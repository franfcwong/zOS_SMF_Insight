{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ce4f80-98f1-468e-90fa-d6bc4e82a091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd5ddea4d60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "from dash import Dash, dcc, html, Input, Output, dash_table, no_update  \n",
    "import plotly.express as px\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import dash_bootstrap_components as dbc\n",
    "from functools import reduce\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css',\n",
    "                        dbc.icons.BOOTSTRAP]\n",
    "\n",
    "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "questionIcon = '<i class=\"bi-question-octagon-fill\" style=\"font-size: 1rem; color: grey;\"></i>'\n",
    "alertIcon = '<i class=\"bi-exclamation-octagon-fill\" style=\"font-size: 1rem; color: red;\"></i>'\n",
    "checkIcon = '<i class=\"bi-check-circle-fill\" style=\"font-size: 1rem; color: green;\"></i>'\n",
    "\n",
    "def get_svg_octagon(pct):\n",
    "    if np.isnan(pct):\n",
    "        return questionIcon\n",
    "    else:\n",
    "        return f'<i class=\"bi-octagon-fill\" style=\"font-size: {pct}2rem; color: bs-info;\"></i>'\n",
    "\n",
    "with open('/home/wong/Documents/newsmf110s2.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "columns_wanted = ['header',\n",
    "                  'dfha06ds', \n",
    "                  'dfhdsgds', 'dfhxmgds', 'dfhsmsds',\n",
    "                  'dfha14ds', 'dfha20ds', 'dfha06ds', 'dfhnqgds', 'dfha17ds',\n",
    "                  'dfha08ds', 'dfhd2gds', 'dfhd2rds', 'dfhmqgds']\n",
    "\n",
    "filteredData =[]\n",
    "df_dict = {}\n",
    "dictfilt = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y)])\n",
    "\n",
    "for item in data:\n",
    "    newItem = dictfilt(item, columns_wanted)\n",
    "    filteredData.append(newItem)\n",
    "    \n",
    "# Flatten data\n",
    "data_norm = pd.json_normalize(filteredData)\n",
    "data_norm['dateTime'] = data_norm['header.dateTime'].map(lambda x:\n",
    "                              datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    \n",
    "# process dfha06ds\n",
    "df_a06ds = None\n",
    "if 'dfha06ds' in data_norm.keys():\n",
    "    df_a06ds = pd.json_normalize([elem for elem in filteredData \n",
    "                                  if (\"dfha06ds\" in elem.keys() and\n",
    "                                      isinstance(elem['dfha06ds'], list))], \n",
    "                             'dfha06ds',\n",
    "                             meta=[['header', 'smfstprn'],\n",
    "                                   ['header', 'dateTime'],\n",
    "                                   ['header', 'sysId']],\n",
    "                             errors='ignore',\n",
    "                             record_prefix='dfha06ds.')\n",
    "    df_a06ds = df_a06ds[['header.smfstprn','header.dateTime','header.sysId',\n",
    "                         'dfha06ds.a06teot','dfha06ds.a06csvc',\n",
    "                         'dfha06ds.a06tete','dfha06ds.a06teoe']]\n",
    "    df_a06ds['dateTime'] = df_a06ds['header.dateTime'].map(lambda x:\n",
    "                                  datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    df_a06ds.drop('header.dateTime', axis=1)\n",
    "    df_dict[\"a06ds\"] = df_a06ds\n",
    "\n",
    "# process dfhdsgds.dsgtcbm\n",
    "df_dsgds = None\n",
    "if 'dfhdsgds.dsgtcbm' in data_norm.keys():\n",
    "    df_dsgds = pd.json_normalize([elem for elem in filteredData if \"dfhdsgds\" in elem.keys()], \n",
    "                             ['dfhdsgds','dsgtcbm'],\n",
    "                             meta=[['header']],\n",
    "                             errors='ignore',\n",
    "                             record_prefix='dsgtcbm.')\n",
    "    df_dsgds['header.smfstprn'] = df_dsgds.apply(lambda x: x['header']['smfstprn'], axis=1)\n",
    "    df_dsgds['header.sysId'] = df_dsgds.apply(lambda x: x['header']['sysId'], axis=1)\n",
    "    df_dsgds['dateTime'] = df_dsgds.apply(lambda x: x['header']['dateTime'], axis=1)\n",
    "    df_dsgds['dateTime'] = df_dsgds['dateTime'].map(lambda x:\n",
    "                                  datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    df_dsgds['dsgtcbm.dsgact'] = df_dsgds['dsgtcbm.dsgact'].map(lambda x: \n",
    "                                datetime.strptime(x,\"%H:%M:%S.%f\") - \n",
    "                                datetime(1900,1,1))\n",
    "    df_dsgds.drop(columns='header', inplace=True)\n",
    "\n",
    "    df_dsgds = df_dsgds.loc[df_dsgds['dsgtcbm.dsgtcbnm']=='QR'][['header.smfstprn',\n",
    "                                                'dateTime','header.sysId',\n",
    "                                                'dsgtcbm.dsgact']]\n",
    "    df_dict[\"dsgds\"] = df_dsgds\n",
    "\n",
    "\n",
    "# process dfhxmgds\n",
    "df_xmgds = None\n",
    "df_xmgds_1 = None\n",
    "if 'dfhxmgds' in data_norm.keys():\n",
    "    df_xmgds_1 = pd.json_normalize([elem for elem in filteredData \n",
    "                                    if (\"dfhxmgds\" in elem.keys() and\n",
    "                                        isinstance(elem['dfhxmgds'], list))],\n",
    "                             'dfhxmgds',\n",
    "                             meta=[['header','smfstprn'],\n",
    "                                   ['header', 'dateTime'],\n",
    "                                   ['header', 'sysId']],\n",
    "                             errors='ignore',\n",
    "                             record_prefix='dfhxmgds.')\n",
    "if 'dfhxmgds.xmgpat' in data_norm.keys():\n",
    "    if df_xmgds_1 is None:\n",
    "        df_xmgds = data_norm[['header.smfstprn','header.dateTime','header.sysId',\n",
    "                              'dfhxmgds.xmgpat','dfhxmgds.xmgtamxt']].dropna()\n",
    "    else:\n",
    "        df_xmgds = pd.concat([df_xmgds_1[['header.smfstprn','header.dateTime','header.sysId',\n",
    "                                          'dfhxmgds.xmgpat','dfhxmgds.xmgtamxt']], \n",
    "                              data_norm[['header.smfstprn','header.dateTime','header.sysId',\n",
    "                                         'dfhxmgds.xmgpat','dfhxmgds.xmgtamxt']].dropna()], axis=0)\n",
    "else:\n",
    "    df_xmgds = df_xmgds_1\n",
    "\n",
    "if df_xmgds is not None:\n",
    "    df_xmgds['dateTime'] = df_xmgds['header.dateTime'].map(lambda x:\n",
    "                                  datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    df_xmgds = df_xmgds[['header.smfstprn','dateTime','header.sysId',\n",
    "                         'dfhxmgds.xmgpat','dfhxmgds.xmgtamxt']]\n",
    "        \n",
    "    df_dict[\"xmgds\"] = df_xmgds\n",
    "\n",
    "\n",
    "# process dfhsmsds.smsbody\n",
    "df_smsds = None\n",
    "if 'dfhsmsds.smsbody' in data_norm.keys():\n",
    "    df_smsds = pd.json_normalize([elem for elem in filteredData if \"dfhsmsds\" in elem.keys()],\n",
    "                             ['dfhsmsds', 'smsbody'],\n",
    "                             meta=[['header']],\n",
    "                             errors='ignore',\n",
    "                             record_prefix='dfhsmsds.')\n",
    "    df_smsds['header.smfstprn'] = df_smsds.apply(lambda x: x['header']['smfstprn'], axis=1)\n",
    "    df_smsds['header.sysId'] = df_smsds.apply(lambda x: x['header']['sysId'], axis=1)\n",
    "    df_smsds['dateTime'] = df_smsds.apply(lambda x: x['header']['dateTime'], axis=1)\n",
    "    df_smsds.drop(columns='header', inplace=True)\n",
    "    df_smsds['dateTime'] = df_smsds['dateTime'].map(lambda x:\n",
    "                                  datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    \n",
    "    df_smsds = df_smsds[['header.smfstprn','dateTime','header.sysId',\n",
    "                         'dfhsmsds.smsdsaname','dfhsmsds.smssos']]\n",
    "    condition1 = (df_smsds['dfhsmsds.smsdsaname'].str.startswith('E'))\n",
    "    condition2 = (df_smsds['dfhsmsds.smsdsaname'].str.startswith('E','G'))\n",
    "    df_smsds['dfhsmsds.smssos_EDSAs'] = df_smsds[condition1]['dfhsmsds.smssos']\n",
    "    df_smsds['dfhsmsds.smssos_DSAs'] = df_smsds[~condition2]['dfhsmsds.smssos']\n",
    "    df_smsds.drop('dfhsmsds.smssos', axis=1)\n",
    "\n",
    "    df_dict[\"smsds\"] = df_smsds\n",
    "\n",
    "    \n",
    "# process dfha17ds\n",
    "df_a17ds_1 = None\n",
    "df_a17ds = None\n",
    "if 'dfha17ds' in data_norm.keys():\n",
    "    df_a17ds_1 = pd.json_normalize([elem for elem in filteredData \n",
    "                                    if ( \"dfha17ds\" in elem.keys() and \n",
    "                                        isinstance(elem['dfha17ds'], list))],\n",
    "                             record_path=['dfha17ds'],\n",
    "                             meta=[['header','smfstprn'],\n",
    "                                   ['header', 'dateTime'],\n",
    "                                   ['header', 'sysId']],\n",
    "                             errors='ignore',\n",
    "                             record_prefix='dfha17ds.')\n",
    "if 'dfha17ds.a17dshsw' in data_norm.keys():\n",
    "    if df_a17ds_1 is None:\n",
    "        df_a17ds = data_norm[['header.smfstprn','header.dateTime',\n",
    "                              'header.sysId','dfha17ds.a17dshsw']].dropna()\n",
    "    else:\n",
    "        df_a17ds = pd.concat([df_a17ds_1[['header.smfstprn','header.dateTime',\n",
    "                                          'header.sysId','dfha17ds.a17dshsw']],\n",
    "                              data_norm[['header.smfstprn','header.dateTime',\n",
    "                                         'header.sysId','dfha17ds.a17dshsw']].dropna()], axis=0)\n",
    "elif df_a17ds_1 is not None:\n",
    "    df_a17ds = df_a17ds_1\n",
    "\n",
    "if df_a17ds is not None:        \n",
    "    df_a17ds['dateTime'] = df_a17ds['header.dateTime'].map(lambda x:\n",
    "                                  datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    df_a17ds.drop('header.dateTime',axis=1)\n",
    "    df_a17ds = df_a17ds[['header.smfstprn','dateTime',\n",
    "                         'header.sysId','dfha17ds.a17dshsw']]\n",
    "    df_dict[\"a17ds\"] = df_a17ds    \n",
    "    \n",
    "# process dfha08ds\n",
    "df_a08ds_1 = None\n",
    "if 'dfha08ds' in data_norm.keys():\n",
    "    df_a08ds_1 = pd.json_normalize([elem for elem in filteredData \n",
    "                                    if ( \"dfha08ds\" in elem.keys() and \n",
    "                                        isinstance(elem['dfha08ds'], list))],\n",
    "                             record_path=['dfha08ds'],\n",
    "                             meta=[['header','smfstprn'],\n",
    "                                   ['header','dateTime'],\n",
    "                                   ['header','sysId']],\n",
    "                             errors='ignore',\n",
    "                             record_prefix='dfha08ds.')\n",
    "if 'dfha08ds.a08bkhsw' in data_norm.keys():\n",
    "    if df_a08ds_1 is None:\n",
    "        df_a08ds = data_norm[['header.smfstprn','header.dateTime',\n",
    "                              'header.sysId','dfha08ds.a08bkhsw']].dropna()\n",
    "    else:\n",
    "        df_a08ds = pd.concat([df_a08ds_1[['header.smfstprn','header.dateTime',\n",
    "                                          'header.sysId','dfha08ds.a08bkhsw']],\n",
    "                              data_norm[['header.smfstprn','header.dateTime',\n",
    "                                         'header.sysId','dfha08ds.a08bkhsw']].dropna()], axis=0)\n",
    "else:\n",
    "    df_a08ds = df_a08ds_1\n",
    "\n",
    "if df_a08ds is not None:        \n",
    "    df_a08ds['dateTime'] = df_a08ds['header.dateTime'].map(lambda x:\n",
    "                                  datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    df_a08ds = df_a08ds[['header.smfstprn','dateTime',\n",
    "                         'header.sysId','dfha08ds.a08bkhsw']]\n",
    "else:\n",
    "    df_a08ds = pd.DataFrame(columns=['header.sysId','header.smfstprn','dateTime','dfha08ds.a08bkhsw'])\n",
    "df_dict[\"a08ds\"] = df_a08ds\n",
    "\n",
    "# process dfha14ds\n",
    "df_a14ds = None\n",
    "df_a14ds_1 = None\n",
    "if 'dfha14ds' in data_norm.keys():\n",
    "    df_a14ds_1 = pd.json_normalize([elem for elem in filteredData \n",
    "                                    if ( \"dfha14ds\" in elem.keys() and \n",
    "                                        isinstance(elem['dfha14ds'], list))],\n",
    "                             record_path=['dfha14ds'],\n",
    "                             meta=[['header','smfstprn'],\n",
    "                                   ['header','dateTime'],\n",
    "                                   ['header','sysId']],\n",
    "                             errors='ignore',\n",
    "                             record_prefix='dfha14ds.')\n",
    "if 'dfha14ds.a14estas' in data_norm.keys():\n",
    "    if df_a14ds_1 is None:\n",
    "        df_a14ds = data_norm[['header.smfstprn','header.dateTime',\n",
    "                              'header.sysId','dfha14ds.a14estas',\n",
    "                               'dfha14ds.a14estaq','dfha14ds.a14estaf',\n",
    "                               'dfha14ds.a14estao','dfha14ds.a14estam']].dropna()\n",
    "    else:\n",
    "        df_a14ds = pd.concat([df_a14ds_1[['header.smfstprn','header.dateTime',\n",
    "                                          'header.sysId','dfha14ds.a14estas',\n",
    "                                          'dfha14ds.a14estaq','dfha14ds.a14estaf',\n",
    "                                          'dfha14ds.a14estao','dfha14ds.a14estam']],\n",
    "                              data_norm[['header.smfstprn','header.dateTime',\n",
    "                                         'header.sysId','dfha14ds.a14estas',\n",
    "                                          'dfha14ds.a14estaq','dfha14ds.a14estaf',\n",
    "                                          'dfha14ds.a14estao','dfha14ds.a14estam']].dropna()], axis=0)\n",
    "else:\n",
    "    df_a14ds = df_a14ds_1\n",
    "\n",
    "if df_a14ds is not None:        \n",
    "    df_a14ds['dateTime'] = df_a14ds['header.dateTime'].map(lambda x:\n",
    "                                  datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    df_a14ds = df_a14ds[['header.smfstprn','dateTime','header.sysId',\n",
    "                         'dfha14ds.a14estas','dfha14ds.a14estaq',\n",
    "                         'dfha14ds.a14estaf','dfha14ds.a14estao',\n",
    "                         'dfha14ds.a14estam']] \n",
    "    df_dict[\"a14ds\"] = df_a14ds\n",
    "\n",
    "\n",
    "# process dfhnqgds.nqgbody\n",
    "df_nqgds = None\n",
    "if 'dfhnqgds.nqgbody' in data_norm.keys():\n",
    "    df_nqgds = pd.json_normalize([elem for elem in filteredData if \"dfhnqgds\" in elem.keys()],\n",
    "                             ['dfhnqgds', 'nqgbody'],\n",
    "                             meta=[['header']],\n",
    "                             errors='ignore',\n",
    "                             record_prefix='dfhnqgds.')\n",
    "    df_nqgds['header.smfstprn'] = df_nqgds.apply(lambda x: x['header']['smfstprn'], axis=1)\n",
    "    df_nqgds['header.sysId'] = df_nqgds.apply(lambda x: x['header']['sysId'], axis=1)\n",
    "    df_nqgds['dateTime'] = df_nqgds.apply(lambda x: x['header']['dateTime'], axis=1)\n",
    "    df_nqgds.drop(columns='header', inplace=True)\n",
    "    df_nqgds['dateTime'] = df_nqgds['dateTime'].map(lambda x:\n",
    "                              datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "    df_nqgds['dfhnqgds.nqgtnqwt'] = df_nqgds['dfhnqgds.nqgtnqwt'].map(lambda x: \n",
    "                                datetime.strptime(x,\"%H:%M:%S.%f\") - \n",
    "                                datetime(1900,1,1))\n",
    "    df_nqgds['dfhnqgds.nqggnqwt'] = df_nqgds['dfhnqgds.nqggnqwt'].map(lambda x: \n",
    "                                datetime.strptime(x,\"%H:%M:%S.%f\") - \n",
    "                                datetime(1900,1,1))\n",
    "    df_nqgds['dfhnqgds.nqgnqwt'] = df_nqgds['dfhnqgds.nqgtnqwt'] + df_nqgds['dfhnqgds.nqggnqwt']\n",
    "    df_nqgds['dfhnqgds.avg_nqgnqwt'] = df_nqgds['dfhnqgds.nqgnqwt']/df_nqgds['dfhnqgds.nqgtnqsi']\n",
    "    df_nqgds = df_nqgds[['header.smfstprn','dateTime','header.sysId',\n",
    "                         'dfhnqgds.nqgtnqsi',\n",
    "                         'dfhnqgds.nqgtnqwt',\n",
    "                         'dfhnqgds.nqggnqwt',\n",
    "                         'dfhnqgds.nqgnqwt',\n",
    "                         'dfhnqgds.avg_nqgnqwt']]\n",
    "    df_dict[\"nqgds\"] = df_nqgds\n",
    "\n",
    "df = reduce(lambda left, right: pd.merge(left,right, \n",
    "                            on=['header.sysId','header.smfstprn','dateTime'],\n",
    "                            how='outer'), df_dict.values())\n",
    "\n",
    "dff = df.groupby(['header.sysId','header.smfstprn']).agg({\n",
    "            'dfha06ds.a06teot' : 'sum',\n",
    "            'dfha06ds.a06csvc' : 'sum',\n",
    "            'dfha06ds.a06tete' : 'sum',\n",
    "            'dfha06ds.a06teoe' : 'sum',\n",
    "            'dfhsmsds.smssos_EDSAs' : 'sum',\n",
    "            'dfhsmsds.smssos_DSAs' : 'sum',\n",
    "            'dfhnqgds.nqgtnqsi' : 'sum',\n",
    "            'dfhnqgds.nqgtnqwt': 'sum',\n",
    "            'dfhnqgds.nqggnqwt': 'sum',\n",
    "            'dsgtcbm.dsgact': 'max',\n",
    "            'dfhxmgds.xmgpat': 'max',\n",
    "            'dfhxmgds.xmgtamxt': 'max',\n",
    "            'dfha17ds.a17dshsw': 'max',\n",
    "            'dfha08ds.a08bkhsw': 'max',\n",
    "            'dfha14ds.a14estas': 'max',\n",
    "            'dfha14ds.a14estaq': 'max',\n",
    "            'dfha14ds.a14estaf': 'max',\n",
    "            'dfha14ds.a14estao': 'max',\n",
    "            'dfha14ds.a14estam': 'max',\n",
    "            }).reset_index()\n",
    "\n",
    "dff['dfhnqgds.nqgnqwt'] = dff[['dfhnqgds.nqgtnqwt','dfhnqgds.nqggnqwt']].sum(axis=1)\n",
    "dff['dfhnqgds.avg_nqgnqwt'] = dff['dfhnqgds.nqgnqwt']/dff['dfhnqgds.nqgtnqsi']\n",
    "dff['SOS on any of EDSAs']=dff['dfhsmsds.smssos_EDSAs'].apply(lambda \n",
    "                                x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))\n",
    "dff['SOS on any of DSAs']=dff['dfhsmsds.smssos_DSAs'].apply(lambda \n",
    "                                x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))\n",
    "dff['Term Xmit Err']=dff['dfha06ds.a06tete'].apply(lambda \n",
    "                                x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))\n",
    "dff['Term Xact Err']=dff['dfha06ds.a06teoe'].apply(lambda \n",
    "                                x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))\n",
    "dff['Term Storage Violation']=dff['dfha06ds.a06csvc'].apply(lambda \n",
    "                                x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))\n",
    "dff['Waits for VSAM File Str']=dff['dfha17ds.a17dshsw'].apply(lambda \n",
    "                                x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))\n",
    "   \n",
    "dff['Waits for LSR Pool Str']=dff['dfha08ds.a08bkhsw'].apply(lambda \n",
    "                                x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))\n",
    "\n",
    "dff['Fail Link Alloc']=dff['dfha14ds.a14estaf'].apply(lambda \n",
    "                                x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))\n",
    "dff['Fail Other Reasons']=dff['dfha14ds.a14estao'].apply(lambda \n",
    "                                x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))\n",
    "dff['Max O/S Alloc']=dff['dfha14ds.a14estam'].apply(lambda \n",
    "                                x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))\n",
    "dff['Times at MAXTASK']=dff['dfhxmgds.xmgtamxt'].apply(lambda \n",
    "                            x:alertIcon if x > 0 else(checkIcon if x == 0 else questionIcon))    \n",
    "dff['Total Enq Issued Rank']=dff['dfhnqgds.nqgtnqsi'].rank(pct=True)\n",
    "dff['Total Enq Issued'] = dff['Total Enq Issued Rank'].map(lambda\n",
    "                                x:get_svg_octagon(x))\n",
    "dff['Peak Active User Xacts Rank']=dff['dfhxmgds.xmgpat'].rank(pct=True)\n",
    "dff['Peak Active User Xacts'] = dff['Peak Active User Xacts Rank'].map(lambda\n",
    "                                x:get_svg_octagon(x))\n",
    "dff['Term Xacts Rank']=dff['dfha06ds.a06teot'].rank(pct=True)\n",
    "dff['Term Xacts'] = dff['Term Xacts Rank'].map(lambda\n",
    "                                x:get_svg_octagon(x))\n",
    "dff['Total Alloc Rank']=dff['dfha14ds.a14estas'].rank(pct=True)\n",
    "dff['Total Alloc'] = dff['Total Alloc Rank'].map(lambda\n",
    "                                x:get_svg_octagon(x))\n",
    "dff['Queue Alloc Rank']=dff['dfha14ds.a14estaq'].rank(pct=True)\n",
    "dff['Queue Alloc'] = dff['Queue Alloc Rank'].map(lambda\n",
    "                                x:get_svg_octagon(x))\n",
    "    \n",
    "dff['Highest QR TCB Rank'] = dff['dsgtcbm.dsgact'].rank(pct=True)\n",
    "dff['Highest QR TCB'] = dff['Highest QR TCB Rank'].map(lambda\n",
    "                                x:get_svg_octagon(x))\n",
    "dff['Total Enq Wait Time Rank'] = dff['dfhnqgds.nqgnqwt'].rank(pct=True)\n",
    "dff['Total Enq Wait Time'] = dff['Total Enq Wait Time Rank'].map(lambda\n",
    "                                x:get_svg_octagon(x))\n",
    "dff['Avg Enq Wait Time Rank'] = dff['dfhnqgds.avg_nqgnqwt'].rank(pct=True)\n",
    "dff['Avg Enq Wait Time'] = dff['Avg Enq Wait Time Rank'].map(lambda\n",
    "                                x:get_svg_octagon(x))\n",
    "dff = dff.rename({'header.sysId': 'LPAR',\n",
    "                  'header.smfstprn': 'Applid'},axis=1)\n",
    "\n",
    "dff[\"id\"] = dff.index\n",
    "\n",
    "columns = {'LPAR' : 'header.sysId', 'Applid': 'header.smfstprn', \n",
    "                    'SOS on any of EDSAs': 'dfhsmsds.smssos_EDSAs', \n",
    "                    'SOS on any of DSAs': 'dfhsmsds.smssos_DSAs',\n",
    "                    'Term Xmit Err': 'dfha06ds.a06tete', \n",
    "                    'Term Xact Err': 'dfha06ds.a06teoe',\n",
    "                    'Term Storage Violation': 'dfha06ds.a06csvc', \n",
    "                    'Waits for VSAM File Str': 'dfha17ds.a17dshsw',\n",
    "                    'Waits for LSR Pool Str': 'dfha08ds.a08bkhsw',\n",
    "                    'Fail Link Alloc': 'dfha14ds.a14estaf', \n",
    "                    'Fail Other Reasons': 'dfha14ds.a14estao', \n",
    "                    'Max O/S Alloc': 'dfha14ds.a14estam',\n",
    "                    'Times at MAXTASK': 'dfhxmgds.xmgtamxt', \n",
    "                    'Total Enq Issued': 'dfhnqgds.nqgtnqsi',\n",
    "                    'Peak Active User Xacts': 'dfhxmgds.xmgpat', \n",
    "                    'Term Xacts': 'dfha06ds.a06teot',\n",
    "                    'Total Alloc': 'dfha14ds.a14estas', \n",
    "                    'Queue Alloc': 'dfha14ds.a14estaq',\n",
    "                    'Highest QR TCB': 'dsgtcbm.dsgact', \n",
    "                    'Total Enq Wait Time': 'dfhnqgds.nqgnqwt',\n",
    "                    'Avg Enq Wait Time': 'dfhnqgds.avg_nqgnqwt'}\n",
    "\n",
    "initial_active_cell = {\"row\": 0, \"column\": 0, \"column_id\": \"Peak Active User Xacts\", \"row_id\": 0}\n",
    "\n",
    "app.layout = html.Div(\n",
    "    [\n",
    "        html.Div(\n",
    "            [\n",
    "                html.H3(\"CICS Healthiness\", style={\"textAlign\":\"center\"}),\n",
    "                dash_table.DataTable(\n",
    "                    id=\"table\",\n",
    "                    columns=[{\"name\": c, \"id\": c, \"presentation\":\"markdown\"} for c in columns.keys()],\n",
    "                    data=dff.to_dict(\"records\"),\n",
    "                    page_size=10,\n",
    "                    sort_action=\"native\",\n",
    "                    active_cell=initial_active_cell,\n",
    "                    markdown_options={'html': True},\n",
    "                    style_cell = {\n",
    "\n",
    "                        'minWidth': '30px','width': '30px', 'maxWidth': '60px',\n",
    "                        'whiteSpace': 'normal'\n",
    "                    },\n",
    "                ),\n",
    "            ],\n",
    "            style={\"margin\": 50},\n",
    "            # className=\"five columns\"\n",
    "        ),\n",
    "        html.Div(id=\"output-graph\", className=\"six columns\"),\n",
    "    ],\n",
    "    className=\"row\"\n",
    ")\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "      Output(\"output-graph\", \"children\"),\n",
    "      Input(\"table\", \"active_cell\"),\n",
    "  )\n",
    "def cell_clicked(active_cell):\n",
    "    if active_cell is None:\n",
    "        return no_update\n",
    "    \n",
    "    row = active_cell[\"row_id\"]\n",
    "\n",
    "    applid = dff.at[row, \"Applid\"]\n",
    "    \n",
    "    lparid = dff.at[row, \"LPAR\"]\n",
    "    \n",
    "    col = active_cell[\"column_id\"]\n",
    "\n",
    "    df_s = df[\n",
    "                    (df[\"header.sysId\"]==lparid)  &\n",
    "                    (df[\"header.smfstprn\"]==applid)]\n",
    "       \n",
    "    y = columns[col] if col in columns.keys() else columns['Peak Active User Xacts']\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df_s, x=\"dateTime\", y=y, title=\" \".join([applid, col])\n",
    "    )\n",
    "    fig.update_layout(title={\"font_size\": 20},  title_x=0.5, margin=dict(t=190, r=15, l=5, b=5))\n",
    "    #fig.update_traces(line=dict(color=color[y]))\n",
    "\n",
    "    return dcc.Graph(figure=fig)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "app.run_server(debug=True, mode='inline') #, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801fd598-c80c-4b9f-822d-c18bcd72fb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
